{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936a988a",
   "metadata": {},
   "source": [
    "# ReviewLens AI: Model Development and Validation\n",
    "\n",
    "**Author:** Kevin Della Piazza\n",
    "**Date:** October 2025\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook documents the development and validation process for the core AI models of the ReviewLens platform. The objective is to test a suite of four distinct NLP models on a sample of the dataset to validate their effectiveness and to prototype the foundational Python code for the production AWS Lambda functions. This notebook serves as a professional artifact detailing the data science workflow, from data preparation to model evaluation.\n",
    "\n",
    "### AI Models Tested:\n",
    "1.  **Sentiment Analysis:** For overall positive/negative classification.\n",
    "2.  **Zero-Shot Classification:** For dynamic topic tagging.\n",
    "3.  **Aspect-Based Sentiment Analysis (ABSA):** For fine-grained sentiment on specific features.\n",
    "4.  **Topic Modeling:** For discovering latent themes in the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24226fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "import re\n",
    "import stopit\n",
    "\n",
    "# --- AI Libraries ---\n",
    "from transformers import pipeline\n",
    "from pyabsa import ATEPCCheckpointManager\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# --- Validation ---\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "# Suppress ignorable warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09428c",
   "metadata": {},
   "source": [
    "## 2. Data Loading, Cleaning & Sanitization\n",
    "We load the raw dataset and apply the cleaning steps that will be replicated in our AWS pipeline. A text sanitization step is included to prevent errors from special characters, making the code more robust for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c177440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "try:\n",
    "    # Use a relative path to ensure portability\n",
    "    file_path = '../data/reviews.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows.\")\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    print(\"Starting data cleaning...\")\n",
    "    df_cleaned = df.drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "    df_cleaned.dropna(subset=['Review Text'], inplace=True)\n",
    "    df_cleaned['Title'] = df_cleaned['Title'].fillna('')\n",
    "    df_cleaned['full_review_text'] = df_cleaned['Title'] + ' ' + df_cleaned['Review Text']\n",
    "    df_cleaned.dropna(subset=['Division Name', 'Department Name', 'Class Name'], inplace=True)\n",
    "    \n",
    "    # --- Text Sanitization Function ---\n",
    "    def sanitize_text(text):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        # Replace ampersand, which was identified as a \"poison pill\" for ABSA\n",
    "        text = text.replace('&', 'and')\n",
    "        # Remove non-printable control characters\n",
    "        text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "        return text\n",
    "\n",
    "    # Apply sanitization to the main text column\n",
    "    df_cleaned['full_review_text'] = df_cleaned['full_review_text'].apply(sanitize_text)\n",
    "    print(f\"Data cleaned and sanitized. {len(df_cleaned)} rows remaining.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: reviews.csv not found. Please ensure the file is in the 'data/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe0673",
   "metadata": {},
   "source": [
    "## 3. Sample Creation\n",
    "To ensure rapid development and testing, we create two distinct samples:\n",
    "* A **small sample (100 reviews)** for fast, iterative tests on row-level models.\n",
    "* A **larger sample (2000 reviews)** for Topic Modeling, which requires a larger corpus to generate meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53855954",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_cleaned' in locals() and not df_cleaned.empty:\n",
    "    # Small sample for fast, iterative tests\n",
    "    df_sample_fast = df_cleaned.sample(100, random_state=42).copy()\n",
    "    print(f\"Created 'df_sample_fast' with {len(df_sample_fast)} reviews for rapid testing.\")\n",
    "\n",
    "    # Larger sample required for meaningful Topic Modeling\n",
    "    df_sample_topic = df_cleaned.sample(2000, random_state=42).copy()\n",
    "    print(f\"Created 'df_sample_topic' with {len(df_sample_topic)} reviews for Topic Modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27debe93",
   "metadata": {},
   "source": [
    "## 4. AI Model Development & Validation\n",
    "\n",
    "### A Note on Model Constraints (Token Limits)\n",
    "\n",
    "All models based on the BERT or Transformer architecture (including DistilBERT and `pyabsa`'s underlying models) have a **maximum input size limit**, typically 512 tokens (approx. 350-400 words).\n",
    "\n",
    "In a production environment, passing text longer than this limit will cause the model to fail. To build a robust pipeline, we must **truncate** the input text to this 512-token limit.\n",
    "\n",
    "**Is this a problem?** No. For review analysis, this is an acceptable trade-off. The core sentiment and topics of a review are almost always contained in the first few paragraphs. By truncating the text, we gain massive performance and stability at a negligible cost to accuracy. The `bertopic` model handles this truncation automatically.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1. Sentiment Analysis\n",
    "* **Business Question:** Are our customers generally happy or unhappy?\n",
    "* **Model Chosen:** `distilbert-base-uncased-finetuned-sst-2-english`\n",
    "* **Justification:** Chosen for its excellent balance of speed and accuracy, making it ideal for scalable, serverless environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Sentiment Analysis model...\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def safe_get_sentiment(text):\n",
    "    \"\"\"Applies sentiment analysis with error handling.\"\"\"\n",
    "    try:\n",
    "        return sentiment_pipeline(text[:512])[0]['label']\n",
    "    except Exception as e:\n",
    "        print(f\"--> SENTIMENT FAILED on review: '{text[:50]}...' | Error: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "print(f\"Applying Sentiment Analysis to the fast sample ({len(df_sample_fast)} reviews)...\")\n",
    "df_sample_fast['sentiment_prediction'] = df_sample_fast['full_review_text'].progress_apply(safe_get_sentiment)\n",
    "print(\"Sentiment Analysis complete.\")\n",
    "\n",
    "# --- Quantitative Validation ---\n",
    "print(\"\\n--- Model Validation ---\")\n",
    "# Define \"ground truth\" based on user ratings (e.g., > 3 stars is positive)\n",
    "df_sample_fast['true_sentiment'] = np.where(df_sample_fast['Rating'] > 3, 'POSITIVE', 'NEGATIVE')\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(df_sample_fast['true_sentiment'], df_sample_fast['sentiment_prediction'])\n",
    "print(f\"Sentiment Model Accuracy on Sample: {accuracy:.2%}\")\n",
    "\n",
    "# Display a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(df_sample_fast['true_sentiment'], df_sample_fast['sentiment_prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f361c57",
   "metadata": {},
   "source": [
    "### 4.2. Zero-Shot Classification\n",
    "* **Business Question:** What specific topics (e.g., 'price', 'shipping') are customers talking about?\n",
    "* **Model Chosen:** `typeform/distilbert-base-uncased-mnli`\n",
    "* **Justification:** This model provides a fast and highly accurate zero-shot classification capability. Its efficient \"distilled\" architecture is ideal for a serverless environment, ensuring high performance and scalability. This allows us to dynamically categorize reviews against key business topics (like 'price' or 'shipping') without needing to retrain the model for new categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ce00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Zero-Shot Classification model (typeform/distilbert-base-uncased-mnli)...\")\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\")\n",
    "print(\"Zero-Shot model loaded successfully.\")\n",
    "\n",
    "candidate_labels = ['price', 'quality', 'shipping', 'customer service', 'fit', 'fabric']\n",
    "\n",
    "def safe_get_top_topic(review_text):\n",
    "    \"\"\"Applies zero-shot classification with error handling.\"\"\"\n",
    "    try:\n",
    "        return zero_shot_classifier(review_text[:512], candidate_labels)['labels'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"--> ZERO-SHOT FAILED on review: '{review_text[:50]}...' | Error: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "print(f\"Applying Zero-Shot Classification to the fast sample ({len(df_sample_fast)} reviews)...\")\n",
    "df_sample_fast['zero_shot_topic'] = df_sample_fast['full_review_text'].progress_apply(safe_get_top_topic)\n",
    "print(\"Zero-Shot Classification complete.\")\n",
    "\n",
    "# --- Qualitative Validation ---\n",
    "print(\"\\n--- Qualitative Validation Examples ---\")\n",
    "display(df_sample_fast[['full_review_text', 'zero_shot_topic']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40e9ad",
   "metadata": {},
   "source": [
    "### 4.3. Aspect-Based Sentiment Analysis (ABSA using Zero-Shot)\n",
    "* **Business Question:** When customers discuss a topic, what specific *features* do they like or dislike, and with what sentiment?\n",
    "* **Model Chosen:** `MoritzLaurer/mDeBERTa-v3-base-mnli-xnli` (used via Zero-Shot Classification pipeline)\n",
    "* **Justification:** We employ a reliable Zero-Shot Classification technique using the powerful, multilingual mDeBERTa model to perform ABSA. By providing a curated list of \"Aspect-Sentiment Pairs\" (e.g., 'slow delivery') as candidate labels and enabling multi_label=True, this method accurately identifies all relevant aspects within the review text. It offers high accuracy and stability for extracting fine-grained sentiment in a deployable serverless function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Zero-Shot Classification model for ABSA (MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)...\")\n",
    "# Note: This is a different, more powerful model than the one used for Topic classification\n",
    "absa_zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n",
    "print(\"Zero-Shot ABSA model loaded successfully.\")\n",
    "\n",
    "# --- Define Aspect-Sentiment Pair (ASP) Labels ---\n",
    "# These labels are crucial. They define the specific aspect-sentiment combinations the model will look for.\n",
    "# Customize this list based on the expected domain (e.g., clothing, electronics).\n",
    "aspect_sentiment_labels = [\n",
    "    'slow delivery', 'fast delivery', 'damaged box', 'good packaging',\n",
    "    'good quality', 'poor quality', 'defective item',\n",
    "    'good fit', 'tight fit', 'loose fit', 'wrong size',\n",
    "    'good price', 'expensive', 'value for money',\n",
    "    'soft fabric', 'rough fabric', 'nice color', 'wrong color',\n",
    "    'good customer service', 'poor customer service'\n",
    "]\n",
    "# Define the confidence score threshold\n",
    "score_threshold = 0.6\n",
    "\n",
    "def safe_get_aspects_zeroshot(review_text):\n",
    "    \"\"\"\n",
    "    Applies zero-shot classification with multi_label=True to find all relevant\n",
    "    Aspect-Sentiment Pairs above a certain threshold.\n",
    "    \"\"\"\n",
    "    if not isinstance(review_text, str) or not review_text.strip():\n",
    "        return \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Truncate text\n",
    "        truncated_text = \" \".join(review_text.split()[:400]) # Safe estimate for 512 tokens\n",
    "\n",
    "        # Run inference with multi_label=True\n",
    "        results = absa_zero_shot_classifier(\n",
    "            truncated_text,\n",
    "            aspect_sentiment_labels,\n",
    "            multi_label=True\n",
    "        )\n",
    "\n",
    "        # Filter results based on the score threshold\n",
    "        matching_aspects = []\n",
    "        for label, score in zip(results['labels'], results['scores']):\n",
    "            if score >= score_threshold:\n",
    "                # Store as \"label (score)\" for validation, or just \"label\" for production\n",
    "                matching_aspects.append(f\"{label} ({score:.2f})\")\n",
    "\n",
    "        if not matching_aspects:\n",
    "            return \"N/A\"\n",
    "\n",
    "        # Return comma-separated string\n",
    "        return \", \".join(matching_aspects)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--> ZERO-SHOT ABSA FAILED on review: '{review_text[:50]}...' | Error: {e}\")\n",
    "        return \"PREDICTION_ERROR\" # Use a distinct error code\n",
    "\n",
    "print(f\"Applying Zero-Shot ABSA to the fast sample ({len(df_sample_fast)} reviews)...\")\n",
    "# Apply the new function\n",
    "df_sample_fast['aspects'] = df_sample_fast['full_review_text'].progress_apply(safe_get_aspects_zeroshot)\n",
    "print(\"Zero-Shot ABSA complete.\")\n",
    "\n",
    "# --- Qualitative Validation & Error Checking ---\n",
    "print(\"\\n--- Qualitative Validation Examples (Aspects) ---\")\n",
    "# Display reviews where aspects were successfully found\n",
    "display(df_sample_fast[~df_sample_fast['aspects'].isin([\"N/A\", \"PREDICTION_ERROR\"])][['full_review_text', 'aspects']].head())\n",
    "\n",
    "failed_reviews = df_sample_fast[df_sample_fast['aspects'] == \"PREDICTION_ERROR\"]\n",
    "if not failed_reviews.empty:\n",
    "    print(f\"\\nWARNING: {len(failed_reviews)} reviews failed during Zero-Shot ABSA prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51996fd9",
   "metadata": {},
   "source": [
    "### 4.4. Topic Modeling\n",
    "* **Business Question:** What are the hidden, high-level themes of conversation across all reviews?\n",
    "* **Model Chosen:** `bertopic`\n",
    "* **Justification:** BERTopic leverages transformer embeddings to find semantically coherent topics, which are more interpretable than traditional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting Topic Modeling on the larger sample ({len(df_sample_topic)} reviews)...\")\n",
    "docs = df_sample_topic['full_review_text'].tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# 1. Create the list of stop words to ignore\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# 2. Create the \"employee\" (CountVectorizer) and give it the stop word list\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# 3. Initialize BERTopic and tell it to use our specific \"employee\"\n",
    "topic_model = BERTopic(language=\"english\", vectorizer_model=vectorizer, verbose=False)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "print(\"Topic Modeling complete!\")\n",
    "\n",
    "# --- Qualitative Validation ---\n",
    "print(\"\\n--- Discovered Topics Summary ---\")\n",
    "\n",
    "# Display a summary of the most prominent topics. Topic -1 contains outliers and can be ignored.\n",
    "# We also show only the top 10 most frequent topics (plus Topic -1)\n",
    "display(topic_model.get_topic_info().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4ff2b",
   "metadata": {},
   "source": [
    "#### **How to Read This Table:**\n",
    "\n",
    "* **`Topic`**: The ID number for the theme.\n",
    "    * **Topic `-1` (The Outliers):** This is the **outlier group**. It contains all unique, one-off reviews that don't fit into a larger theme. **You should ignore Topic -1 when analyzing trends.**\n",
    "* **`Count`**: The number of reviews in that theme. This shows you how popular a topic is.\n",
    "* **`Representation`**: The **new, clean keywords** that best describe the theme. After remove the stop-words, we can see the true essence of the conversation (e.g., `[jeans, fit, pants, love]`).\n",
    "* **`Representative_Docs`**: A full, real review from that group, which provides the ultimate context. **Read this column to give the topic a human-readable name.**\n",
    "\n",
    "**Example Interpretation:**\n",
    "* **Topic 0:** Keywords might be `[dress, love, fit, size]`. This is the **\"Dress Reviews\"** cluster.\n",
    "* **Topic 1:** Keywords might be `[jeans, fit, pants, comfortable]`. This is the **\"Pants & Jeans\"** cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c3f99",
   "metadata": {},
   "source": [
    "## 5. Final Result & Conclusion\n",
    "The sample DataFrame is now fully enriched with insights from all AI models. This validated logic is ready to be integrated into the production Lambda functions. The final data structure provides a multi-dimensional view of each customer review, enabling a rich, interactive analysis on the final dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder columns for a clean final view\n",
    "# Note: 'bertopic_id' is not in this fast sample; it's a corpus-level insight.\n",
    "final_columns = [\n",
    "    'full_review_text', \n",
    "    'sentiment_prediction', \n",
    "    'zero_shot_topic',\n",
    "    'aspects',\n",
    "    'Rating',\n",
    "    'true_sentiment'\n",
    "]\n",
    "\n",
    "print(\"--- Final Enriched Sample DataFrame (from fast sample) ---\")\n",
    "display(df_sample_fast[final_columns].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1794ef",
   "metadata": {},
   "source": [
    "The DataFrame above represents the **final enriched product** for a *single sample* of reviews. \n",
    "Each new column provides a different layer of AI-driven insight:\n",
    "\n",
    "* **`full_review_text` (Input):** The raw, sanitized text (Title + Review Text) that was fed into the pipeline.\n",
    "* **`sentiment_prediction` (AI Layer 1):** The **overall sentiment** of the entire review (e.g., `POSITIVE`/`NEGATIVE`), as determined by our *Sentiment Lambda*.\n",
    "* **`zero_shot_topic` (AI Layer 2):** The **primary topic** of the review, dynamically classified into one of our predefined business categories (e.g., `price`, `quality`), as determined by our *Zero-Shot Lambda*.\n",
    "* **`aspects` (AI Layer 3):** The **most granular and actionable analysis**. This extracts the specific *features* (aspects) mentioned and the sentiment attached *to each one* (e.g., `fabric (NEGATIVE)`), as determined by our *ABSA Lambda*.\n",
    "* **`Rating` (Input):** The original 1-5 star rating provided by the user.\n",
    "* **`true_sentiment` (Validation Only):** A \"ground truth\" column created *only* in this notebook to validate our model. This column is not part of the final production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe675611",
   "metadata": {},
   "source": [
    "## 6. Conclusive Analysis: How to Interpret and Use These Results\n",
    "\n",
    "The final, enriched table is a powerful business tool. Its value is unlocked by asking specific questions and combining the AI-generated columns to get answers.\n",
    "\n",
    "Here is a practical guide on how a business stakeholder would use this data.\n",
    "\n",
    "### Business Question 1: \"How are we doing?\"\n",
    "\n",
    "**Analysis:** Use the `sentiment_prediction` column.\n",
    "\n",
    "By aggregating this column, a manager can get an instant, high-level KPI of overall customer happiness. This dashboard metric can be tracked weekly to spot immediate changes in brand perception.\n",
    "\n",
    "**Example Insight:**\n",
    "* \"This week, our Positive Sentiment Score dropped by 8%.\"\n",
    "\n",
    "---\n",
    "### Business Question 2: \"Our sentiment dropped... *Why*?\"\n",
    "\n",
    "**Analysis:** Combine `sentiment_prediction` and `zero_shot_topic`.\n",
    "\n",
    "This is the first level of diagnosis. A manager can filter for all reviews where `sentiment_prediction == 'NEGATIVE'` and then create a bar chart of the `zero_shot_topic` column.\n",
    "\n",
    "**Example Insight:**\n",
    "* \"Our sentiment dropped because of a 60% spike in complaints. By filtering for those complaints, we see that **72% of them are about 'shipping'**.\"\n",
    "* **Action:** The business now knows exactly where the problem is. They don't need to waste time investigating 'price' or 'quality'; they have a clear priority.\n",
    "\n",
    "---\n",
    "### Business Question 3: \"Okay, shipping is the problem. But *what about* shipping?\"\n",
    "\n",
    "**Analysis:** Combine `zero_shot_topic` and `aspects`.\n",
    "\n",
    "This is the most powerful, granular insight. The manager can now filter for all reviews where `zero_shot_topic == 'shipping'` and analyze the `aspects` column.\n",
    "\n",
    "**Example Insight:**\n",
    "* By creating a word cloud from the `aspects` in this segment, they see that the most common phrases are **`delivery time (NEGATIVE)`** and **`box (NEGATIVE)`**.\n",
    "* **Action:** The business has its final answer. The problem isn't the cost of shipping; it's that the courier is slow and the packaging is getting damaged. They can now take surgical action, like renegotiating with their courier or improving their packaging materials.\n",
    "\n",
    "---\n",
    "### Business Question 4: \"What problems are we not even aware of?\"\n",
    "\n",
    "**Analysis:** Use the `bertopic_id` from the full, corpus-level analysis (as seen in section 4.4).\n",
    "\n",
    "This analysis is performed by the `stitcher-lambda` on the entire dataset. It automatically clusters reviews by \"hidden themes\" that we didn't define in advance.\n",
    "\n",
    "**Example Insight:**\n",
    "* By examining the `topic_model.get_topic_info()` output, the manager spots a new cluster (e.g., `[jeans, fit, pants, comfortable]`).\n",
    "* **Action:** The manager realizes that a huge number of customers (Count: 219) are discussing jeans not just in terms of price or quality (our predefined topics), but specifically in terms of fit and comfort. This hidden theme was previously invisible.\n",
    "\n",
    "---\n",
    "### Final Conclusion\n",
    "\n",
    "This notebook has validated the logic for each of these four analytical layers. The deployed AWS pipeline is built to perform this exact multi-layer analysis at scale, transforming raw text feedback from a \"cost center\" (something to be stored) into a **strategic asset** (something to be queried)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
